Lmod has detected the following error: These module(s) or extension(s) exist
but cannot be loaded as requested: "binutils/2.28-GCCcore-6.4.0"
   Try: "module spider binutils/2.28-GCCcore-6.4.0" to see how to load the
module(s).



Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Set SLURM handle signals.

  | Name                | Type               | Params
-----------------------------------------------------------
0 | image_embedding     | VisionEncoder      | 86.4 M
1 | text_embedding      | TextEncoder        | 109 M 
2 | transformer_encoder | TransformerEncoder | 33.1 M
3 | MLP_head            | Linear             | 23.5 M
4 | dropout             | Dropout            | 0     
5 | softmax             | Softmax            | 0     
6 | accuracy            | Accuracy           | 0     
-----------------------------------------------------------
56.6 M    Trainable params
195 M     Non-trainable params
252 M     Total params
1,009.710 Total estimated model params size (MB)
Hyper Parameters:  Namespace(dataset='visual-genome', image_embedding='pure-attention')
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]Traceback (most recent call last):
  File "main.py", line 46, in <module>
    trainer.fit(model, data_module)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 740, in fit
    self._call_and_handle_interrupt(
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1311, in _run_train
    self._run_sanity_check(self.lightning_module)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1375, in _run_sanity_check
    self._evaluation_loop.run()
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 110, in advance
    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 140, in run
    self.on_run_start(*args, **kwargs)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 86, in on_run_start
    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py", line 121, in _update_dataloader_iter
    dataloader_iter = enumerate(data_fetcher, batch_idx)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 199, in __iter__
    self.prefetching(self.prefetch_batches)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 258, in prefetching
    self._fetch_next_batch()
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py", line 300, in _fetch_next_batch
    batch = next(self.dataloader_iter)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 530, in __next__
    data = self._next_data()
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1224, in _next_data
    return self._process_data(data)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1250, in _process_data
    data.reraise()
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
ValueError: Caught ValueError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/torch/utils/data/dataset.py", line 471, in __getitem__
    return self.dataset[self.indices[idx]]
  File "/data/s3894479/BachelorsProject/datasets/visualgenome/VisualGenomeQuestionsAnswers.py", line 82, in __getitem__
    answer = self.load_target(index)
  File "/data/s3894479/BachelorsProject/datasets/visualgenome/VisualGenomeQuestionsAnswers.py", line 78, in load_target
    return tensor(self.tokenizer(self.data[index][2]).input_ids[0])
  File "/data/s3894479/.envs/python386-bachelors/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2437, in __call__
    raise ValueError(
ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).



###############################################################################
Peregrine Cluster
Job 23341942 for user 's3894479'
Finished at: Wed Mar 23 10:16:47 CET 2022

Job details:
============

Job ID              : 23341942
Name                : MultiModalTransformer
User                : s3894479
Partition           : gpu
Nodes               : pg-gpu13
Number of Nodes     : 1
Cores               : 12
Number of Tasks     : 1
State               : FAILED
Submit              : 2022-03-23T09:25:13
Start               : 2022-03-23T10:05:29
End                 : 2022-03-23T10:16:46
Reserved walltime   : 3-00:00:00
Used walltime       :   00:11:17
Used CPU time       :   00:09:37 (efficiency:  7.10%)
% User (Computation): 97.77%
% System (I/O)      :  2.23%
Mem reserved        : 64G
Max Mem (Node/step) : 5.87G (pg-gpu13, per node)
Full Max Mem usage  : 5.87G
Total Disk Read     : 6.06M
Total Disk Write    : 51.51K
Average GPU usage   : 0.0% (pg-gpu13)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/introduction/scientific_output

################################################################################
